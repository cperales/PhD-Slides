% This file was created with JabRef 2.10.
% Encoding: UTF-8

% Sherman Morrison
@article{hager1989updating,
  title={Updating the inverse of a matrix},
  author={Hager, William W},
  journal={SIAM review},
  volume={31},
  number={2},
  pages={221--239},
  year={1989},
  publisher={SIAM}
}


@InProceedings{an2007face,
  Title                    = {Face recognition using kernel ridge regression},
  Author                   = {An, Senjian and Liu, Wanquan and Venkatesh, Svetha},
  Booktitle                = {Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on},
  Year                     = {2007},
  Organization             = {IEEE},
  Pages                    = {1--7}
}

@Article{Bbeiman1996,
  Title                    = {{Bagging Predictors}},
  Author                   = {Bbeiman, Leo},
  Journal                  = {Machine Learning},
  Year                     = {1996},
  Pages                    = {123--140},
  Volume                   = {24},

  Abstract                 = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  File                     = {:home/cperales/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bbeiman - 1996 - Bagging Predictors.pdf:pdf},
  Keywords                 = {Aggregation Bootstrap,Averaging,Combining},
  Owner                    = {cperales},
  Timestamp                = {2018.01.24},
  Url                      = {https://link.springer.com/content/pdf/10.1007{\%}2FBF00058655.pdf}
}

@Article{Freund1997,
  Title                    = {{A Short Introduction to Boosting}},
  Author                   = {Freund, Yoav and Schapire, Robert E},
  Journal                  = {Journal of Japanese Society for Artificial Intelligence},
  Year                     = {1999},
  Number                   = {5},
  Pages                    = {771--780},
  Volume                   = {14},

  Abstract                 = {Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the un-derlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting's relationship to support-vector machines. Some examples of recent applications of boosting are also described.},
  File                     = {:home/cperales/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Freund, Schapire - 1999 - A Short Introduction to Boosting.pdf:pdf},
  Owner                    = {cperales},
  Timestamp                = {2018.01.24},
  Url                      = {http://www.yorku.ca/gisweb/eats4400/boost.pdf}
}

@Article{hoerl1970ridge,
  Title                    = {Ridge regression: Biased estimation for nonorthogonal problems},
  Author                   = {Hoerl, Arthur E and Kennard, Robert W},
  Journal                  = {Technometrics},
  Year                     = {1970},
  Number                   = {1},
  Pages                    = {55--67},
  Volume                   = {12},

  Publisher                = {Taylor \& Francis Group}
}

@Article{huang2012extreme,
  Title                    = {Extreme learning machine for regression and multiclass classification},
  Author                   = {Huang, Guang-Bin and Zhou, Hongming and Ding, Xiaojian and Zhang, Rui},
  Journal                  = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {513--529},
  Volume                   = {42},

  Publisher                = {IEEE}
}

@Article{huang2006extreme,
  Title                    = {Extreme learning machine: theory and applications},
  Author                   = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
  Journal                  = {Neurocomputing},
  Year                     = {2006},
  Number                   = {1-3},
  Pages                    = {489--501},
  Volume                   = {70},

  Publisher                = {Elsevier}
}

@Article{samuel1959some,
  Title                    = {Some studies in machine learning using the game of checkers},
  Author                   = {Samuel, Arthur L},
  Journal                  = {IBM Journal of research and development},
  Year                     = {1959},
  Number                   = {3},
  Pages                    = {210--229},
  Volume                   = {3},

  Publisher                = {IBM}
}

% Encoding: UTF-8
@article{Park2015housing,
title = "Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data",
journal = "Expert Systems with Applications",
volume = "42",
number = "6",
pages = "2928 - 2934",
year = "2015",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2014.11.040",
url = "http://www.sciencedirect.com/science/article/pii/S0957417414007325",
author = "Byeonghwa Park and Jae Kwon Bae",
keywords = "Housing price index, Housing price prediction model, Machine learning algorithms, C4.5, RIPPER, Naïve Bayesian, AdaBoost",
abstract = "House sales are determined based on the Standard & Poor’s Case-Shiller home price indices and the housing price index of the Office of Federal Housing Enterprise Oversight (OFHEO). These reflect the trends of the US housing market. In addition to these housing price indices, the development of a housing price prediction model can greatly assist in the prediction of future housing prices and the establishment of real estate policies. This study uses machine learning algorithms as a research methodology to develop a housing price prediction model. To improve the accuracy of housing price prediction, this paper analyzes the housing data of 5359 townhouses in Fairfax County, Virginia, gathered by the Multiple Listing Service (MLS) of the Metropolitan Regional Information Systems (MRIS). We develop a housing price prediction model based on machine learning algorithms such as C4.5, RIPPER, Naïve Bayesian, and AdaBoost and compare their classification accuracy performance. We then propose an improved housing price prediction model to assist a house seller or a real estate agent make better informed decisions based on house price valuation. The experiments demonstrate that the RIPPER algorithm, based on accuracy, consistently outperforms the other models in the performance of housing price prediction."
}

@article{kourou2015machine,
  title={Machine learning applications in cancer prognosis and prediction},
  author={Kourou, Konstantina and Exarchos, Themis P and Exarchos, Konstantinos P and Karamouzis, Michalis V and Fotiadis, Dimitrios I},
  journal={Computational and structural biotechnology journal},
  volume={13},
  pages={8--17},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{tian2010new,
  title={A new modeling method based on bagging ELM for day-ahead electricity price prediction},
  author={Tian, Huixin and Meng, Bo},
  booktitle={2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)},
  pages={1076--1079},
  year={2010},
  organization={IEEE}
}

@inproceedings{yuan2018commandersong,
  title={Commandersong: A systematic approach for practical adversarial voice recognition},
  author={Yuan, Xuejing and Chen, Yuxuan and Zhao, Yue and Long, Yunhui and Liu, Xiaokang and Chen, Kai and Zhang, Shengzhi and Huang, Heqing and Wang, XiaoFeng and Gunter, Carl A},
  booktitle={27th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 18)},
  pages={49--64},
  year={2018}
}

@article{pena2014object,
  title={Object-based image classification of summer crops with machine learning methods},
  author={Pe{\~n}a, Jos{\'e} M and Guti{\'e}rrez, Pedro A and Herv{\'a}s-Mart{\'\i}nez, C{\'e}sar and Six, Johan and Plant, Richard E and L{\'o}pez-Granados, Francisca},
  journal={Remote Sensing},
  volume={6},
  number={6},
  pages={5019--5041},
  year={2014},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@book{kohonen2012,
  title={Self-organization and associative memory},
  author={Kohonen, Teuvo},
  volume={8},
  year={2012},
  publisher={Springer Science \& Business Media}
}


@article{churn_binary,
  title={Optimum profit-driven churn decision making: innovative artificial neural networks in telecom industry},
  author={Jafari-Marandi, Ruholla and Denton, Joshua and Idris, Adnan and Smith, Brian K and Keramati, Abbas},
  journal={Neural Computing and Applications},
  volume={32},
  number={18},
  pages={14929--14962},
  year={2020},
  publisher={Springer}
}

@inproceedings{hepatitis_binary,
  title={Using rules to analyse bio-medical data: a comparison between C4. 5 and PCL},
  author={Li, Jinyan and Wong, Limsoon},
  booktitle={International Conference on Web-Age Information Management},
  pages={254--265},
  year={2003},
  organization={Springer}
}

@inproceedings{car_multiclass,
  title={Knowledge acquisition and explanation for multi-attribute decision making},
  author={Bohanec, Marko and Rajkovic, Vladislav},
  booktitle={8th Intl Workshop on Expert Systems and their Applications},
  pages={59--78},
  year={1988}
}

@misc{mnist_multiclass,
  title={Mnist handwritten digit database. AT\&T Labs},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  year={2010}
}

@article{student_mutliclass,
  title={Educational data mining and analysis of students’ academic performance using WEKA},
  author={Hussain, Sadiq and Dahan, Neama Abdulaziz and Ba-Alwib, Fadl Mutaher and Ribata, Najoua},
  journal={Indonesian Journal of Electrical Engineering and Computer Science},
  volume={9},
  number={2},
  pages={447--459},
  year={2018}
}


@article{Perrone1995,
abstract = {This paper presents a general theoretical framework for ensemble methods\nof constructing significantly improved regression estimates. Given\na population of regression estimators, we construct a hybrid estimator\nwhich is as good or better in the MSE sense than any estimator in\nthe population. We argue that the ensemble method presented has\nseveral properties: 1) It efficiently uses all the networks of a\npopulation - none of the networks need be discarded. 2) It efficiently\nuses all the available data for training without over-fitting. 3)\nIt inherently performs regularization by smoothing in functional\nspace which helps to avoid over-fitting. 4) It utilizes local minima\nto construct improved estimates whereas other neural network algorithms\nare hindered by local minima. 5) It is ideally suited for parallel\ncomputation. 6) It leads to a very useful and natural measure of\nthe number of distinct estimators in a population. 7) The optimal\nparameters of the ensemble estimator are given in closed form.\\\nExperimental results are provided which show that the ensemble method\ndramatically improves neural network performance on difficult real-world\noptical character recognition tasks.},
author = {PERRONE, MICHAEL P. and COOPER, LEON N.},
doi = {10.1142/9789812795885_0025},
file = {:home/carlos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/PERRONE, COOPER - 2010 - When networks disagree Ensemble methods for hybrid neural networks.pdf:pdf},
journal = {BROWN UNIV PROVIDENCE RI INST FOR BRAIN AND NEURAL SYSTEMS},
mendeley-groups = {Negative Correlation},
month = {feb},
pages = {342--358},
title = {{When networks disagree: Ensemble methods for hybrid neural networks}},
url = {http://www.worldscientific.com/doi/abs/10.1142/9789812795885_0025},
year = {2010}
}

@InBook{ncl_definition,
  pages     = {715--715},
  title     = {Negative Correlation Learning},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_581},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_581},
}

@Book{nilsson2009quest,
  title     = {The quest for artificial intelligence},
  publisher = {Cambridge University Press},
  year      = {2009},
  author    = {Nilsson, Nils J},
}

@Book{Goodfellow2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Book{mitchell2006discipline,
  title     = {The discipline of machine learning},
  publisher = {Carnegie Mellon University, School of Computer Science, Machine Learning~…},
  year      = {2006},
  author    = {Mitchell, Tom Michael},
  volume    = {9},
}

@Book{bishop2006pattern,
  title     = {Pattern recognition and machine learning},
  publisher = {springer},
  year      = {2006},
  author    = {Bishop, Christopher M},
}

@InBook{bias_variance_definition,
  pages     = {100--101},
  title     = {Bias Variance Decomposition},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_74},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_74},
}

@Article{kuncheva2003measures,
  author    = {Kuncheva, Ludmila I and Whitaker, Christopher J},
  title     = {Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy},
  journal   = {Machine learning},
  year      = {2003},
  volume    = {51},
  number    = {2},
  pages     = {181--207},
  publisher = {Springer},
}

@InBook{ensemble,
  pages     = {312--320},
  title     = {Ensemble Learning},
  publisher = {Springer US},
  year      = {2010},
  author    = {Brown, Gavin},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_252},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_252},
}

@InProceedings{ye2012empirical,
  author       = {Ye, Ren and Suganthan, Ponnuthurai N},
  title        = {Empirical comparison of bagging-based ensemble classifiers},
  booktitle    = {2012 15th International Conference on Information Fusion},
  year         = {2012},
  pages        = {917--924},
  organization = {IEEE},
}

@InBook{supervised_dict,
  pages     = {941--941},
  title     = {Supervised Learning},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_803},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_803},
}

@InBook{reinforcement_dict,
  pages     = {849--851},
  title     = {Reinforcement Learning},
  publisher = {Springer US},
  year      = {2010},
  author    = {Stone, Peter},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_714},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_714},
}

@InBook{unsupervised_dict,
  pages     = {1009--1009},
  title     = {Unsupervised Learning},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_867},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_867},
}

@InBook{parameter_estimation,
  chapter   = {Statistics},
  pages     = {522--523},
  title     = {Review of particle physics},
  publisher = {IOP Publishing Ltd.},
  year      = {2016},
  author    = {Particle Data Group and others},
  volume    = {40},
  number    = {10},
  doi       = {10.1088/1674-1137/40/10/100001},
}

@InBook{noise,
  chapter    = {Robust Linear Regression},
  pages      = {302},
  title      = {Non-Convex Optimization for Machine Learning},
  publisher  = {Now Publishers Inc.},
  year       = {2017},
  author     = {Jain, Prateek and Kar, Purushottam},
  volume     = {10},
  number     = {3–4},
  address    = {Hanover, MA, USA},
  month      = dec,
  doi        = {10.1561/2200000058},
  issn       = {1935-8237},
  issue_date = {4 12 2017},
  journal    = {Found. Trends Mach. Learn.},
  numpages   = {195},
  url        = {https://doi.org/10.1561/2200000058},
}

@Book{conant1947on,
  title     = {On understanding science : an historical approach},
  publisher = {Yale University Press G. Cumberlege, Oxford University Press},
  year      = {1947},
  author    = {Conant, James},
  address   = {New Haven London},
  isbn      = {978-0300136555},
}

@InBook{hypothesis_britannica,
  chapter   = {Scientific Hypothesis},
  title     = {Encyclopedia Britannica},
  publisher = {Encyclopedia Britannica, Inc.},
  year      = {2018},
  author    = {Kara Rogers},
  abstract  = {Scientific hypothesis, an idea that proposes a tentative explanation about a phenomenon or a narrow set of phenomena observed in the natural world. The two primary features of a scientific hypothesis are falsifiability and testability, which are reflected in an “If…then” statement summarizing the idea and in the ability to be supported or refuted through observation and experimentation. The notion of the scientific hypothesis as both falsifiable and testable was advanced in the mid-20th century by Austrian-born British philosopher Karl Popper.},
  url       = {https://www.britannica.com/science/scientific-hypothesis},
}

@Book{russell2010artificial,
  title     = {Artificial intelligence : a modern approach},
  publisher = {Prentice Hall},
  year      = {2010},
  author    = {Russell, Stuart},
  address   = {Upper Saddle River, New Jersey},
  isbn      = {9780136042594},
}

@InBook{zero_one_loss,
  pages     = {1031--1031},
  title     = {Zero-One Loss},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_884},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_884},
}

@Article{Kingma2015AdamAM,
  author  = {Diederik P. Kingma and Jimmy Ba},
  title   = {Adam: A Method for Stochastic Optimization},
  journal = {CoRR},
  year    = {2015},
  volume  = {abs/1412.6980},
}

@Book{Witten2005,
  title     = {2nd Data Mining: Practical Machine Learning Tools and Techniques},
  publisher = {Elsevier},
  year      = {2005},
  author    = {Witten, Ian H. and Frank, Eibe},
  series    = {Data Management Systems},
  isbn      = {0120884070},
  keywords  = {bayesian-network-models, book, clustering, decision-tree, genetic-algorithm, machine-learning, naive-bayes, weka},
}

@Article{Hastie2009,
  author        = {Zhu, Ji and Rosset, Saharon and Zou, Hui and Hastie, Trevor and Rosset, Saharon and Zhu, Ji and Zou, Hui and Rosset, Saharon and Zou, Hui and Hastie, Trevor},
  title         = {{Multi-class AdaBoost}},
  journal       = {Statistics and Its Interface},
  year          = {2009},
  volume        = {2},
  number        = {3},
  pages         = {349--360},
  issn          = {19387989},
  abstract      = {Boosting has been a very successful technique for solving the two-class classification problem. In going from two-class to multi-class classification, most algorithms have been restricted to reducing the multi-class classification problem to multiple two-class problems. In this paper, we develop a new algorithm that directly extends the AdaBoost algorithm to the multi-class case without reducing it tomultiple two-class problems. We show that the proposed multi-class AdaBoost algorithm is equivalent to a forward stagewise additive modeling algorithm that minimizes a novel exponential loss for multi-class classification. Furthermore, we show that the exponential loss is a member of a class of Fisher-consistent loss functions for multi-class classification. As shown in the paper, the new algorithm is extremely easy to implement and is highly competitive in terms of misclassification error rate.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.4310/SII.2009.v2.n3.a8},
  eprint        = {arXiv:1011.1669v3},
  file          = {:home/carlos/MEGAsync/Art{\'{i}}culos/Ensemble/samme algorithm.pdf:pdf},
  isbn          = {1751-5823},
  keywords      = {Boosting,Exponential loss,Multi-class classification,Stagewise modeling,boosting,exponential loss,multi-class classification,stagewise modeling},
  pmid          = {21816105},
  url           = {https://web.stanford.edu/$\sim$hastie/Papers/samme.pdf http://www.intlpress.com/site/pub/pages/journals/items/sii/content/vols/0002/0003/a008/},
}

@Article{Freund1999,
  author          = {{Freund Yoav} and Schapire, Robert E and Freund, Yoav and Schapire, Robert E},
  title           = {{A Short Introduction to Boosting}},
  journal         = {Journal of Japanese Society for Artificial Intelligence},
  year            = {1999},
  volume          = {300},
  number          = {7},
  pages           = {293--296},
  issn            = {10450823},
  abstract        = {Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the un-derlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting's relationship to support-vector machines. Some examples of recent applications of boosting are also described.},
  archiveprefix   = {arXiv},
  arxivid         = {arXiv:1508.01136v1},
  doi             = {citeulike-article-id:765005},
  eprint          = {arXiv:1508.01136v1},
  file            = {:home/carlos/MEGAsync/Art{\'{i}}culos/Ensemble/boost.pdf:pdf},
  isbn            = {3540440119},
  mendeley-groups = {Ensembles,HAIS 2018},
  pmid            = {22345901},
  url             = {http://www.yorku.ca/gisweb/eats4400/boost.pdf},
}

@Article{Bbeiman1996,
  author          = {Breiman, Leo},
  title           = {{Bagging predictions}},
  journal         = {Machine Learning},
  year            = {1996},
  volume          = {24},
  number          = {2},
  pages           = {123--140},
  month           = {aug},
  issn            = {08856125},
  abstract        = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  doi             = {10.1023/A:1018054314350},
  file            = {:home/carlos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging predictions.pdf:pdf},
  keywords        = {Aggregation Bootstrap,Averaging,Combining},
  mendeley-groups = {Ensembles,HAIS 2018},
  url             = {http://link.springer.com/10.1023/A:1018054314350},
}

@InProceedings{Zhao2012,
  author    = {Zhao, Jun and Liang, Zhu and Yang, Yong},
  title     = {{Parallelized incremental support vector machines based on MapReduce and Bagging technique}},
  booktitle = {Proceedings of 2012 IEEE International Conference on Information Science and Technology, ICIST 2012},
  year      = {2012},
  abstract  = {One of the mainstream research fields in learning from empirical data by support vector machines (SVM) is an implementation of the incremental learning schemes when the training dataset is huge. Moreover, the challenge of applying incremental SVMs on huge data sets comes from the fact that the amount of computer memory and learning time required along with the amount of dataset increased. In this paper, a parallelized incremental SVM (PISVM) learning algorithm for huge data is proposed. The parallel programming model of MapReduce is introduced and combined with incremental learning method. Each individual SVM is independently trained based on the randomly selected training samples via bootstrap technique, and learns from the new samples independently also. The final decision is made according to the majority voting by all SVMs. Experiment results on UCI standard data sets show that the training time can be reduced and the accuracy can be ensured for the proposed algorithm. {\textcopyright} 2012 IEEE.},
  doi       = {10.1109/ICIST.2012.6221655},
  isbn      = {9781457703454},
}

@InProceedings{Ko2008,
  author          = {Ko, A. H.R and Sabourin, R. and de Oliveira, L. E.S and {de Souza Britto Jr}, A.},
  title           = {{The Implication of Data Diversity for a Classifier-free Ensemble Selection in Random Subspaces}},
  booktitle       = {Pattern Recognition, 2008. ICPR 2008. 19th International Conference on},
  year            = {2008},
  pages           = {1--5},
  abstract        = {The evaluation of road-kill spatial patterns is an important tool to identify priority locations for mitigation measures aiming to reduce wildlife mortality on roads. Single target or multi-species approaches are usually adopted to implement such measures, although their success must be evaluated. We aim to test if road-kill hotspots are coincident among different vertebrate groups. If this is true, data on accidents from one group could be used to plan measures applicable to other groups. We identified hotspots using five different grouping criteria: vertebrate classes (reptiles, mammals or birds), body size (large or small), species commonness (common or rare), type of locomotion (flying or non-flying), and time of activity (preferably nocturnal/crepuscular or diurnal). We analyzed data from road-kill surveys on four roads in southern Brazil, each one with at least one year of monitoring. Using SIRIEMA software, we performed a modified Ripley's K statistic to recognize scales of road-kill aggregation and we carried out Hotspot analyses to identify the location of road-kill aggregations for each group described above in each road. To test for similarity in hotspot location among different groups we performed an association test using correlation as resemblance measure. Hotspot analyses and association tests were done using different scales to evaluate scale effect on similarities. Correlation results between groups presented low values in small scales although had a tendency to increase with increasing scales. Our results show that road-kill hotspots are different among groups, especially when analyzed in small scales. We suggest that, for a successful biodiversity approach in mitigation, one should first select general hotspots in large scales and then identify specific hotspots in small scales to implement specific measures. These findings are relevant in a context of road networks already implemented, where mitigation measures are being planned to reduce their impact on wildlife.},
  doi             = {10.1177/0333102415610877},
  isbn            = {9781424421756},
  issn            = {0333-1024},
  mendeley-groups = {Diversity},
}

@Book{lindberg2007the,
  title     = {The beginnings of western science : the European scientific tradition in philosophical, religious, and institutional context, prehistory to A.D. 1450},
  publisher = {University of Chicago Press},
  year      = {2007},
  author    = {Lindberg, David},
  address   = {Chicago},
  isbn      = {978-0-226-48205-7},
}

@Article{Drabkin1938,
  author    = {Israel E. Drabkin},
  title     = {Notes on the Laws of Motion in Aristotle},
  journal   = {The American Journal of Philology},
  year      = {1938},
  volume    = {59},
  number    = {1},
  pages     = {60},
  doi       = {10.2307/290584},
  publisher = {{JSTOR}},
  url       = {https://doi.org/10.2307/290584},
}

@InBook{spirkin1975dialectical,
  chapter   = {Philosophy As A World-View And A Methodology},
  pages     = {5--47},
  title     = {Dialectical materialism},
  publisher = {Springer},
  year      = {1975},
  author    = {Spirkin, A},
  booktitle = {Themes in Soviet Marxist Philosophy},
}

@Article{DeHaro2019,
  author    = {Sebastian De Haro},
  title     = {Science and Philosophy: A Love{\textendash}Hate Relationship},
  journal   = {Foundations of Science},
  year      = {2019},
  volume    = {25},
  number    = {2},
  pages     = {297--314},
  month     = aug,
  doi       = {10.1007/s10699-019-09619-2},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.1007/s10699-019-09619-2},
}

@Book{holton2001physics,
  title     = {Physics, the human adventure : from Copernicus to Einstein and beyond},
  publisher = {Rutgers University Press},
  year      = {2001},
  author    = {Holton, Gerald},
  address   = {New Brunswick, N.J},
  isbn      = {978-0-8135-2908-0},
}

@Book{christianson2000on,
  title     = {On Tycho's island : Tycho Brahe and his assistants, 1570-1601},
  publisher = {Cambridge University Press},
  year      = {2000},
  author    = {Christianson, J. R.},
  address   = {Cambridge, U.K. New York},
  isbn      = {978-0521650816},
}

@Book{okasha2002philosophy,
  title     = {Philosophy of science : a very short introduction},
  publisher = {Oxford University Press},
  year      = {2002},
  author    = {Okasha, Samir},
  address   = {Oxford New York},
  isbn      = {0192802836},
}

@Book{smith2003theory,
  title     = {Theory and reality : an introduction to the philosophy of science},
  publisher = {University of Chicago Press},
  year      = {2003},
  author    = {Smith, Peter},
  address   = {Chicago},
  isbn      = {0-226-30063-3},
}

@Book{kuhn2012the,
  title     = {The structure of scientific revolutions},
  publisher = {The University of Chicago Press},
  year      = {2012},
  author    = {Kuhn, Thomas},
  address   = {Chicago London},
  isbn      = {9780226458113},
}

@Misc{calculating_clock,
  author       = {Paul A. Freiberger and Michael R. Swaine},
  howpublished = {Encyclopædia Britannica},
  year         = {2016},
  abstract     = {Calculating Clock, the earliest known calculator, built in 1623 by the German astronomer and mathematician Wilhelm Schickard. He described it in a letter to his friend the astronomer Johannes Kepler, and in 1624 he wrote again to explain that a machine that he had commissioned to be built for Kepler was, apparently along with the prototype, destroyed in a fire. He called it a Calculating Clock, which modern engineers have been able to reproduce from details in his letters. Even general knowledge of the clock had been temporarily lost when Schickard and his entire family perished during the Thirty Years’ War.},
  url          = {https://www.britannica.com/technology/Calculating-Clock},
}

@Article{johnston1997making,
  author    = {Johnston, Stephen},
  title     = {Making the arithmometer count},
  journal   = {Bulletin Scientific Instrument Society},
  year      = {1997},
  pages     = {12--21},
  publisher = {SCIENTIFIC INSTRUMENT SOCIETY},
}

@Book{ernst1992,
  title     = {The Calculating Machines: Their history and development},
  publisher = {MIT Press},
  year      = {1992},
  author    = {Martin Ernst},
  editor    = {Peggy A. Kidwell and Michael R. Williams},
}

@InCollection{babbage1982babbage,
  author    = {Babbage, Henry P},
  title     = {Babbage’s analytical engine},
  booktitle = {The Origins of Digital Computers},
  publisher = {Springer},
  year      = {1982},
  pages     = {67--70},
}

@Book{toole2010ada,
  title     = {Ada, the Enchantress of Numbers: Poetical Science},
  publisher = {Betty Alexandra Toole},
  year      = {2010},
  author    = {Toole, Betty Alexandra},
}

@Book{cohen2000,
  title     = {Howard Aiken, Portrait of a computer pioneer},
  publisher = {The MIT Press},
  year      = {2000},
  author    = {Bernard Cohen},
  isbn      = {9780262531795},
}

@Misc{cambridge1953,
  title   = {EDSAC 99},
  journal = {A brief informal history of the Computer Laboratory},
  url     = {https://www.cl.cam.ac.uk/events/EDSAC99/history.html},
}

@InBook{Kaski2010,
  pages     = {886--888},
  title     = {Self-Organizing Maps},
  publisher = {Springer US},
  year      = {2010},
  author    = {Kaski, Samuel},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_746},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_746},
}

@Book{one_hot_encoding,
  title     = {Digital design and computer architecture},
  publisher = {Elsevier/Morgan Kaufmann},
  year      = {2012},
  author    = {Harris, David},
  address   = {Amsterdam Boston},
  isbn      = {978-0-12-394424-5},
}

@InProceedings{zhang2018generalized,
  author    = {Zhang, Zhilu and Sabuncu, Mert},
  title     = {Generalized cross entropy loss for training deep neural networks with noisy labels},
  booktitle = {Advances in neural information processing systems},
  year      = {2018},
  pages     = {8778--8788},
}

@InBook{backpropagation,
  pages     = {73--73},
  title     = {Backpropagation},
  publisher = {Springer US},
  year      = {2010},
  author    = {Munro, Paul},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_51},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_51},
}

@InBook{mean_squared_error,
  pages     = {653--653},
  title     = {Mean Squared Error},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_528},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_528},
}

@InBook{squared_error_loss,
  pages     = {912--912},
  title     = {Squared Error Loss},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_777},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_777},
}

@Book{Kuncheva2014,
  title     = {Combining Pattern Classifiers: Methods and Algorithms},
  publisher = {Wiley},
  year      = {2014},
  author    = {Kuncheva, Ludmila I.},
  volume    = {9781118315231},
  address   = {Hoboken, NJ},
  isbn      = {9781118914564},
  abstract  = {Pattern recognition is everywhere. It is the technology behind automatically identifying fraudulent bank transactions, giving verbal instructions to your mobile phone, predicting oil deposit odds, or segmenting a brain tumour within a magnetic resonance image. A decade has passed since the first edition of this book. Combining classifiers, also known as "classifier ensembles," has flourished into a prolific discipline. Viewed from the top, classifier ensembles reside at the intersection of engineering, computing, and mathematics. Zoomed in, classifier ensembles are fuelled by advances in pattern recognition, machine learning and data mining, among others. An ensemble aggregates the "opinions" of several pattern classifiers in the hope that the new opinion will be better than the individual ones. Vox populi, vox Dei.},
  booktitle = {Combining Pattern Classifiers: Methods and Algorithms: Second Edition},
  doi       = {10.1002/9781118914564},
  file      = {:home/carlos/MEGAsync/Art{\'{i}}culos/Tesis/Books/Combining Pattern Classifiers Methods and Algorithms by Ludmila I. Kuncheva (z-lib.org).pdf:pdf},
  pages     = {1--357},
}

@Book{de1818theorie,
  title     = {Th{\'e}orie analytique des probabilit{\'e}s: Suppl{\'e}ment a la th{\'e}orie analytique des probabilit{\'e}s: F{\'e}vrier 1818},
  publisher = {Courcier},
  year      = {1818},
  author    = {de Laplace, Pierre Simon},
  volume    = {2},
}

@Article{liu1999ensemble,
  author          = {Liu, Yong and Yao, Xin},
  title           = {{Ensemble learning via negative correlation}},
  journal         = {Neural networks},
  year            = {1999},
  volume          = {12},
  number          = {10},
  pages           = {1399--1404},
  issn            = {08936080},
  abstract        = {This paper presents a learning approach, i.e. negative correlation learning, for neural network ensembles. Unlike previous learning approaches for neural network ensembles, negative correlation learning attempts to train individual networks in an ensemble and combines them in the same learning process. In negative correlation learning, all the individual networks in the ensemble are trained simultaneously and interactively through the correlation penalty terms in their error functions. Rather than producing unbiased individual networks whose errors are uncorrelated, negative correlation learning can create negatively correlated networks to encourage specialization and cooperation among the individual networks. Empirical studies have been carried out to show why and how negative correlation learning works. The experimental results show that negative correlation learning can produce neural network ensembles with good generalization ability.},
  doi             = {10.1016/S0893-6080(99)00073-8},
  file            = {:home/carlos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Yao - 1999 - 1-s2.0-S0893608099000738-main.pdf.pdf:pdf},
  keywords        = {Combination method,Correct response set,Correlation,Generalisation,Negative correlation learning,Neural network ensembles,combination method,correct response set,correlation,generalisation,negative correlation learning,neural network ensembles},
  mendeley-groups = {Negative Correlation},
  pmid            = {12662623},
  publisher       = {Elsevier},
}

@Article{Bonab2019,
  author          = {Bonab, Hamed and Can, Fazli},
  title           = {{Less Is More: A Comprehensive Framework for the Number of Components of Ensemble Classifiers}},
  journal         = {IEEE transactions on neural networks and learning systems},
  year            = {2019},
  volume          = {30},
  number          = {9},
  pages           = {2735--2745},
  issn            = {21622388},
  abstract        = {The number of component classifiers chosen for an ensemble greatly impacts the prediction ability. In this paper, we use a geometric framework for a priori determining the ensemble size, which is applicable to most of the existing batch and online ensemble classifiers. There are only a limited number of studies on the ensemble size examining majority voting (MV) and weighted MV (WMV). Almost all of them are designed for batch-mode, hardly addressing online environments. Big data dimensions and resource limitations, in terms of time and memory, make the determination of ensemble size crucial, especially for online environments. For the MV aggregation rule, our framework proves that the more strong components we add to the ensemble, the more accurate predictions we can achieve. For the WMV aggregation rule, our framework proves the existence of an ideal number of components, which is equal to the number of class labels, with the premise that components are completely independent of each other and strong enough. While giving the exact definition for a strong and independent classifier in the context of an ensemble is a challenging task, our proposed geometric framework provides a theoretical explanation of diversity and its impact on the accuracy of predictions. We conduct a series of experimental evaluations to show the practical value of our theorems and existing challenges.},
  archiveprefix   = {arXiv},
  arxivid         = {1709.02925},
  doi             = {10.1109/TNNLS.2018.2886341},
  eprint          = {1709.02925},
  file            = {:home/carlos/MEGAsync/Art{\'{i}}culos/Ensemble/1709.02925.pdf:pdf;:home/carlos/MEGAsync/Art{\'{i}}culos/Diversity/1709.02925-annotated.pdf:pdf},
  mendeley-groups = {Diversity},
}

@Article{Brown2005,
  author          = {Brown, Gavin and Wyatt, Jeremy L. and Tino, Peter and Tiňo, Peter and Tino, Peter},
  title           = {Managing Diversity in Regression Ensembles},
  journal         = {Journal of Machine Learning Research},
  year            = {2005},
  volume          = {6},
  pages           = {1621--1650},
  issn            = {1532-4435},
  abstract        = {Ensembles are a widely used and effective technique in machine learning--their success is commonly attributed to the degree of disagreement, or 'diversity', within the ensemble. For ensembles where the individual estimators output crisp class labels, this 'diversity' is not well understood and remains an open research issue. For ensembles of regression estimators, the diversity can be exactly formulated in terms of the covariance between individual estimator outputs, and the optimum level is expressed in terms of a bias-variance-covariance trade-off. Despite this, most approaches to learning ensembles use heuristics to encourage the right degree of diversity. In this work we show how to explicitly control diversity through the error function. The first contribution of this paper is to show that by taking the combination mechanism for the ensemble into account we can derive an error function for each individual that balances ensemble diversity with individual accuracy. We show the relationship between this error function and an existing algorithm called negative correlation learning, which uses a heuristic penalty term added to the mean squared error function. It is demonstrated that these methods control the bias-variance-covariance trade-off systematically, and can be utilised with any estimator capable of minimising a quadratic error function, for example MLPs, or RBF networks. As a second contribution, we derive a strict upper bound on the coefficient of the penalty term, which holds for any estimator that can be cast in a generalised linear regression framework, with mild assumptions on the basis functions. Finally we present the results of an empirical study, showing significant improvements over simple ensemble learning, and finding that this technique is competitive with a variety of methods, including boosting, bagging, mixtures of experts, and Gaussian processes, on a number of tasks.},
  file            = {:home/carlos/MEGAsync/Art{\'{i}}culos/Negative Correlation/8_brown05a.pdf:pdf;:home/carlos/MEGAsync/Art{\'{i}}culos/Diversity/8_brown05a-annotated.pdf:pdf},
  isbn            = {1532-4435},
  keywords        = {Diversity,Ensemble,Hessian matrix,Negative correlation learning,Neural networks,Regression estimators,learning,statistics & optimisation,theory & algorithms},
  mendeley-groups = {Diversity,Bias and variance,Negative Correlation},
  url             = {http://eprints.pascal-network.org/archive/00004768/},
}

@Book{non_convex_book,
  title      = {Non-Convex Optimization for Machine Learning},
  publisher  = {Now Publishers Inc.},
  year       = {2017},
  author     = {Jain, Prateek and Kar, Purushottam},
  volume     = {10},
  number     = {3–4},
  address    = {Hanover, MA, USA},
  month      = dec,
  chapter    = {Robust Linear Regression},
  doi        = {10.1561/2200000058},
  issn       = {1935-8237},
  issue_date = {4 12 2017},
  journal    = {Found. Trends Mach. Learn.},
  numpages   = {195},
  pages      = {302},
  url        = {https://doi.org/10.1561/2200000058},
}

@Book{theodoridis2009pattern,
  title     = {Pattern recognition},
  publisher = {Academic Press},
  year      = {2009},
  author    = {Theodoridis, Sergios},
  address   = {Burlington, MA London},
  isbn      = {9780080949123},
}

@Article{Huang2012,
  author    = {Huang, Guang-Bin and Zhou, Hongming and Ding, Xiaojian and Zhang, Rui},
  title     = {{Extreme learning machine for regression and multiclass classification}},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics. Part B, Cybernetics},
  year      = {2012},
  volume    = {42},
  number    = {2},
  pages     = {513--29},
  issn      = {1941-0492},
  doi       = {10.1109/TSMCB.2011.2168604},
  isbn      = {1083-4419},
  owner     = {cperales},
  pmid      = {21984515},
  timestamp = {2018.01.23},
}

@Article{Cervellera2016,
  author   = {Cervellera, Cristiano and Macci{\`{o}}, Danilo},
  title    = {{Low-Discrepancy Points for Deterministic Assignment of Hidden Weights in Extreme Learning Machines}},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  year     = {2016},
  volume   = {27},
  number   = {4},
  pages    = {891--896},
  abstract = {The traditional extreme learning machine (ELM) approach is based on a random assignment of the hidden weight values, while the linear coefficients of the output layer are determined analytically. This brief presents an analysis based on geometric properties of the sampling points used to assign the weight values, investigating the replacement of random generation of such values with low-discrepancy sequences (LDSs). Such sequences are a family of sampling methods commonly employed for numerical integration, yielding a more efficient covering of multidimensional sets with respect to random sequences, without the need for any computationally intensive procedure. In particular, we prove that the universal approximation property of the ELM is guaranteed when LDSs are employed, and how an efficient covering affects the convergence positively. Furthermore, since LDSs are generated deterministically, the results do not have a probabilistic nature. Simulation results confirm, in practice, the good theoretical properties given by the combination of ELM with LDSs.},
}

@Article{Henriquez2017,
  author   = {Henr{\'{i}}quez, Pablo A. and Ruz, Gonzalo A.},
  title    = {{Extreme learning machine with a deterministic assignment of hidden weights in two parallel layers}},
  journal  = {Neurocomputing},
  year     = {2017},
  volume   = {226},
  pages    = {109--116},
  abstract = {Extreme learning machine (ELM) is a machine learning technique based on competitive single-hidden layer feedforward neural network (SLFN). However, traditional ELM and its variants are only based on random assignment of hidden weights using a uniform distribution, and then the calculation of the weights output using the least-squares method. This paper proposes a new architecture based on a non-linear layer in parallel by another non-linear layer and with entries of independent weights. We explore the use of a deterministic assignment of the hidden weight values using low-discrepancy sequences (LDSs). The simulations are performed with Halton and Sobol sequences. The results for regression and classification problems confirm the advantages of using the proposed method called PL-ELM algorithm with the deterministic assignment of hidden weights. Moreover, the PL-ELM algorithm with the deterministic generation using LDSs can be extended to other modified ELM algorithms.},
}

@Article{Castano2013,
  author  = {Casta{\~{n}}o, A. and Fern{\'{a}}ndez-Navarro, Francisco and Herv{\'{a}}s-Mart{\'{i}}nez, C.},
  title   = {{PCA-ELM: A robust and pruned extreme learning machine approach based on principal component analysis}},
  journal = {Neural Processing Letters},
  year    = {2013},
  volume  = {37},
  number  = {3},
  pages   = {377--392},
}

@InProceedings{Huang2004,
  author       = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
  title        = {Extreme learning machine: a new learning scheme of feedforward neural networks},
  booktitle    = {2004 IEEE international joint conference on neural networks (IEEE Cat. No. 04CH37541)},
  year         = {2004},
  volume       = {2},
  pages        = {985--990},
  organization = {IEEE},
}

@InProceedings{deng2009regularized,
  author       = {Deng, Wanyu and Zheng, Qinghua and Chen, Lin},
  title        = {Regularized extreme learning machine},
  booktitle    = {2009 IEEE symposium on computational intelligence and data mining},
  year         = {2009},
  pages        = {389--395},
  organization = {IEEE},
}

@InCollection{kohavi1995automatic,
  author    = {Kohavi, Ron and John, George H},
  title     = {Automatic parameter selection by minimizing estimated error},
  booktitle = {Machine Learning Proceedings},
  publisher = {Elsevier},
  year      = {1995},
  pages     = {304--312},
  isbn      = {978-1-55860-377-6},
}

@InProceedings{mackay1996hyperparameters,
  author    = {MacKay, David JC},
  title     = {Hyperparameters: optimize, or integrate out?},
  booktitle = {13th International Workshop on Maximum Entropy and Bayesian Methods},
  year      = {1996},
  volume    = {62},
  pages     = {43--59},
  publisher = {Springer},
}

@Article{bengio2000gradient,
  author    = {Bengio, Yoshua},
  title     = {Gradient-based optimization of hyperparameters},
  journal   = {Neural computation},
  year      = {2000},
  volume    = {12},
  number    = {8},
  pages     = {1889--1900},
  publisher = {MIT Press},
}

@Article{Krogh1995,
  author   = {Krogh, Anders and Vedelsby, Jesper},
  title    = {Neural Network Ensembles, Cross Validation, and Active Learning},
  journal  = {Advances in Neural Information Processing Systems},
  year     = {1995},
  volume   = {7},
  issn     = {10495258},
  abstract = {Learning of continuous valued functions using neural network ensembles (committees) can give improved accuracy, reliable estimation of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members averaged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combination with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1 INTRODUCTION It is well known that a combination of many different predictors can improve predictions. In the neural networks community "ensembles" of neural networks h...},
  doi      = {10.1.1.37.8876},
  isbn     = {0262201046},
}

@InBook{kfold,
  pages     = {249--249},
  title     = {Cross-Validation},
  publisher = {Springer US},
  year      = {2010},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_190},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_190},
}

% Sin publicar

@Article{gncelm,
  author    = {Perales-Gonzalez, Carlos and Fernandez-Navarro, Francisco and Carbonero-Ruz, Mariano and Perez-Rodriguez, Javier},
  title     = {Global Negative Correlation Learning: A unified framework for global optimization of ensemble models},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2021},
  publisher = {IEEE},
}

@Article{ncelm,
  author    = {Perales-Gonzalez, Carlos and Carbonero-Ruz, Mariano and Perez-Rodriguez, Javier and Becerra-Alonso, David and Fernandez-Navarro, Francisco},
  title     = {Negative correlation learning in the extreme learning machine framework},
  journal   = {Neural Computing and Applications},
  year      = {2020},
  pages     = {1--19},
  publisher = {Springer},
}

@Article{reelm,
  author    = {Perales-Gonz{\'a}lez, Carlos and Carbonero-Ruz, Mariano and Becerra-Alonso, David and P{\'e}rez-Rodr{\'\i}guez, Javier and Fern{\'a}ndez-Navarro, Francisco},
  title     = {Regularized ensemble neural networks models in the Extreme Learning Machine framework},
  journal   = {Neurocomputing},
  year      = {2019},
  volume    = {361},
  pages     = {196--211},
  publisher = {Elsevier},
}

@InProceedings{delm,
  author       = {Fern{\'a}ndez-Navarro, Francisco},
  title        = {A Preliminary Study of Diversity in Extreme Learning Machines Ensembles},
  booktitle    = {Hybrid Artificial Intelligent Systems: 13th International Conference, HAIS 2018, Oviedo, Spain, June 20-22, 2018, Proceedings},
  year         = {2018},
  volume       = {10870},
  pages        = {302},
  organization = {Springer},
}

@Article{boston_housing,
  author    = {Harrison Jr, David and Rubinfeld, Daniel L},
  title     = {Hedonic housing prices and the demand for clean air},
  year      = {1978},
  publisher = {Elsevier},
}

@Article{airline_cost,
  author    = {Proctor, Jesse W and Duncan, Julius S},
  title     = {A regression analysis of airline costs},
  journal   = {J. Air L. \& Com.},
  year      = {1954},
  volume    = {21},
  pages     = {282},
  publisher = {HeinOnline},
}

@Article{fish_toxicity,
  author    = {Cassotti, M and Ballabio, D and Todeschini, R and Consonni, V},
  title     = {A similarity-based QSAR model for predicting acute toxicity towards the fathead minnow (Pimephales promelas)},
  journal   = {SAR and QSAR in Environmental Research},
  year      = {2015},
  volume    = {26},
  number    = {3},
  pages     = {217--243},
  publisher = {Taylor \& Francis},
}

@Comment{jabref-meta: databaseType:bibtex;}
